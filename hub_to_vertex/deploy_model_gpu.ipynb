{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-west1\"\n",
    "PROJECT_ID = \"rag-nick\"\n",
    "REPOSITORY = \"highlight-app\"\n",
    "IMAGE = \"highlight-pipeline-gpu-finetuned2\"\n",
    "TAG = \"py310-cu12.3-torch-2.2.0-transformers-4.38.1\"\n",
    "BUCKET_NAME = \"highlight-app-finetuned-storage\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}/paraphrase-MiniLM-L6-v2/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"google-cloud-aiplatform[prediction]>=1.16.0\"\n",
    "!pip install -r huggingface_predictor_gpu/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1010: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1016: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "from huggingface_predictor.predictor import HuggingFacePredictor\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    \"huggingface_predictor\",\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\",\n",
    "    predictor=HuggingFacePredictor,\n",
    "    requirements_path=\"huggingface_predictor/requirements.txt\",\n",
    "    base_image=\"--platform=linux/amd64 python:3.10-slim AS build\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/Users/haiyiluo/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\",\n",
      "    \"us-east4-docker.pkg.dev\": \"gcloud\",\n",
      "    \"us-west1-docker.pkg.dev\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-west1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [highlight-app]\n",
      "Waiting for operation [projects/rag-nick/locations/us-west1/operations/52955bb8\n",
      "-0d31-4223-a195-ca04f23676d5] to complete...done.                              \n",
      "Created repository [highlight-app].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create highlight-app --repository-format=docker --location={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1010: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:1016: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'paraphrase-MiniLM-L6-v2-finetune-summary'...\n",
      "remote: Enumerating objects: 19, done.\u001b[K\n",
      "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 19 (delta 0), reused 0 (delta 0), pack-reused 3\u001b[K\n",
      "Unpacking objects: 100% (19/19), done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/tonychenxyz/paraphrase-MiniLM-L6-v2-finetune-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1_Pooling\n",
      "a 1_Pooling/config.json\n",
      "a README.md\n",
      "a added_tokens.json\n",
      "a config.json\n",
      "a config_sentence_transformers.json\n",
      "a modules.json\n",
      "a pytorch_model.bin\n",
      "a sentence_bert_config.json\n",
      "a special_tokens_map.json\n",
      "a tokenizer.json\n",
      "a tokenizer_config.json\n",
      "a vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!cd paraphrase-MiniLM-L6-v2-finetune-summary/ && tar zcvf model.tar.gz --exclude flax_model.msgpack --exclude rust_model.ot * && mv model.tar.gz ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://highlight-app-finetuned-storage/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l $REGION -b on -p $PROJECT_ID gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [storage/parallel_composite_upload_enabled].\n",
      "Copying file://model.tar.gz to gs://highlight-app-finetuned-storage/paraphrase-MiniLM-L6-v2/model.tar.gz\n",
      "  Completed files 1/1 | 79.8MiB/79.8MiB | 16.9MiB/s                            \n",
      "\n",
      "Average throughput: 12.7MiB/s\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set storage/parallel_composite_upload_enabled True\n",
    "!gcloud storage cp model.tar.gz $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/522870214401/locations/us-west1/models/2995834934154756096/operations/7954490538424532992\n",
      "Model created. Resource name: projects/522870214401/locations/us-west1/models/2995834934154756096@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/522870214401/locations/us-west1/models/2995834934154756096@1')\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"paraphrase-MiniLM-L6-v2\",\n",
    "    artifact_uri=f\"gs://{BUCKET_NAME}/paraphrase-MiniLM-L6-v2\",\n",
    "    serving_container_image_uri=local_model.get_serving_container_spec().image_uri,\n",
    "    serving_container_environment_variables={\n",
    "        \"HF_TASK\": \"summarization\",\n",
    "        # Optional env var so that `uvicorn` only runs the model in 1 worker\n",
    "        # \"VERTEX_CPR_WEB_CONCURRENCY\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/522870214401/locations/us-west1/endpoints/2549463999603277824/operations/6762162537078194176\n",
      "Endpoint created. Resource name: projects/522870214401/locations/us-west1/endpoints/2549463999603277824\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/522870214401/locations/us-west1/endpoints/2549463999603277824')\n",
      "Deploying model to Endpoint : projects/522870214401/locations/us-west1/endpoints/2549463999603277824\n",
      "Deploy Endpoint model backing LRO: projects/522870214401/locations/us-west1/endpoints/2549463999603277824/operations/1136040702585602048\n",
      "Endpoint model deployed. Resource name: projects/522870214401/locations/us-west1/endpoints/2549463999603277824\n",
      "projects/522870214401/locations/us-west1/endpoints/2549463999603277824\n"
     ]
    }
   ],
   "source": [
    "# endpoint = model.deploy(\n",
    "#     machine_type=\"g2-standard-4\",\n",
    "#     accelerator_type=\"NVIDIA_L4\",\n",
    "#     accelerator_count=1,\n",
    "# )\n",
    "endpoint = model.deploy(machine_type=\"e2-standard-4\")\n",
    "print(endpoint.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequences': ['example sentence', 'Each sentence is converted']}\n",
      "endpoint: \"projects/522870214401/locations/us-west1/endpoints/2549463999603277824\"\n",
      "instances {\n",
      "  struct_value {\n",
      "    fields {\n",
      "      key: \"sequences\"\n",
      "      value {\n",
      "        list_value {\n",
      "          values {\n",
      "            string_value: \"example sentence\"\n",
      "          }\n",
      "          values {\n",
      "            string_value: \"Each sentence is converted\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "<class 'proto.marshal.collections.repeated.RepeatedComposite'>\n",
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "# Initialize AI Platform Prediction client\n",
    "client_options = {\"api_endpoint\": \"us-west1-aiplatform.googleapis.com\"}\n",
    "prediction_client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "# Endpoint resource name\n",
    "endpoint_name = \"projects/522870214401/locations/us-west1/endpoints/2549463999603277824\"\n",
    "\n",
    "# instances = aiplatform_v1.Value()\n",
    "# instances.sequences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "# # Prepare your input data\n",
    "instance = {\"sequences\": [\"example sentence\", \"Each sentence is converted\"]}\n",
    "print(instance)\n",
    "\n",
    "# payload = [{\"instances\": instances}]\n",
    "request = aiplatform_v1.PredictRequest(endpoint=endpoint_name)\n",
    "request.instances.append(instance)\n",
    "print(request)\n",
    "print(type(request.instances))\n",
    "\n",
    "response = prediction_client.predict(request=request)\n",
    "print(response.metadata)\n",
    "print(response.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_type: \"application/json\"\n",
      "data: \"{\\\"sequences\\\": [\\\"example sentence\\\", \\\"Each sentence is converted\\\"]}\"\n",
      "\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "500 {\"detail\":\"The following exception has occurred: KeyError. Arguments: ('instances',).\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/grpc/_channel.py:1176\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1170\u001b[0m (\n\u001b[1;32m   1171\u001b[0m     state,\n\u001b[1;32m   1172\u001b[0m     call,\n\u001b[1;32m   1173\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1174\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1175\u001b[0m )\n\u001b[0;32m-> 1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"{\"detail\":\"The following exception has occurred: KeyError. Arguments: ('instances',).\"}\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.40.202:443 {grpc_message:\"{\\\"detail\\\":\\\"The following exception has occurred: KeyError. Arguments: (\\'instances\\',).\\\"}\", grpc_status:13, created_time:\"2024-04-24T21:56:12.937333-04:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 28\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(http_body)\n\u001b[1;32m     23\u001b[0m request \u001b[38;5;241m=\u001b[39m aiplatform_v1\u001b[38;5;241m.\u001b[39mRawPredictRequest(\n\u001b[1;32m     24\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m     25\u001b[0m     http_body\u001b[38;5;241m=\u001b[39mhttp_body,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:1025\u001b[0m, in \u001b[0;36mPredictionServiceClient.raw_predict\u001b[0;34m(self, request, endpoint, http_body, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mInternalServerError\u001b[0m: 500 {\"detail\":\"The following exception has occurred: KeyError. Arguments: ('instances',).\"}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "# Initialize AI Platform Prediction client\n",
    "client_options = {\"api_endpoint\": \"us-west1-aiplatform.googleapis.com\"}\n",
    "prediction_client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "# Endpoint resource name\n",
    "endpoint = \"projects/522870214401/locations/us-west1/endpoints/2549463999603277824\"\n",
    "\n",
    "# instances = aiplatform_v1.Value()\n",
    "# instances.sequences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "# # Prepare your input data\n",
    "data = {\"sequences\": [\"example sentence\", \"Each sentence is converted\"]}\n",
    "json_data = json.dumps(data)\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json_data.encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "print(http_body)\n",
    "\n",
    "request = aiplatform_v1.RawPredictRequest(\n",
    "    endpoint=endpoint,\n",
    "    http_body=http_body,\n",
    ")\n",
    "\n",
    "response = prediction_client.raw_predict(request)\n",
    "json.loads(response.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployed_model_id: \"4962373053083287552\"\n",
      "model: \"projects/522870214401/locations/us-west1/models/2995834934154756096\"\n",
      "model_version_id: \"1\"\n",
      "model_display_name: \"paraphrase-MiniLM-L6-v2\"\n",
      "\n",
      "<class 'google.cloud.aiplatform_v1.types.prediction_service.PredictResponse'>\n",
      "[]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "print(type(response))\n",
    "print(response.predictions)\n",
    "print(response.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/522870214401/locations/us-west1/models/6110637026434875392\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 The model \"projects/522870214401/locations/us-west1/models/6110637026434875392\" can't be deleted because it's deployed or being deployed at the following endpoint(s): projects/522870214401/locations/us-west1/endpoints/5130308061063282688, projects/522870214401/locations/us-west1/endpoints/8381906992024780800, projects/522870214401/locations/us-west1/endpoints/9183547725696729088. Undeploy the model from all endpoints first and then delete it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/grpc/_channel.py:1176\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1170\u001b[0m (\n\u001b[1;32m   1171\u001b[0m     state,\n\u001b[1;32m   1172\u001b[0m     call,\n\u001b[1;32m   1173\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1174\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1175\u001b[0m )\n\u001b[0;32m-> 1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.FAILED_PRECONDITION\n\tdetails = \"The model \"projects/522870214401/locations/us-west1/models/6110637026434875392\" can't be deleted because it's deployed or being deployed at the following endpoint(s): projects/522870214401/locations/us-west1/endpoints/5130308061063282688, projects/522870214401/locations/us-west1/endpoints/8381906992024780800, projects/522870214401/locations/us-west1/endpoints/9183547725696729088. Undeploy the model from all endpoints first and then delete it.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.165.138:443 {grpc_message:\"The model \\\"projects/522870214401/locations/us-west1/models/6110637026434875392\\\" can\\'t be deleted because it\\'s deployed or being deployed at the following endpoint(s): projects/522870214401/locations/us-west1/endpoints/5130308061063282688, projects/522870214401/locations/us-west1/endpoints/8381906992024780800, projects/522870214401/locations/us-west1/endpoints/9183547725696729088. Undeploy the model from all endpoints first and then delete it.\", grpc_status:9, created_time:\"2024-04-23T15:27:21.881445-04:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# delete model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/cloud/aiplatform/base.py:1368\u001b[0m, in \u001b[0;36mVertexAiResourceNounWithFutureManager.delete\u001b[0;34m(self, sync)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;129m@optional_sync\u001b[39m()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete\u001b[39m(\u001b[38;5;28mself\u001b[39m, sync: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deletes this Vertex AI resource. WARNING: This deletion is\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    permanent.\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;124;03m            be immediately returned and synced when the Future has completed.\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/cloud/aiplatform/base.py:1202\u001b[0m, in \u001b[0;36m_VertexAiResourceNounPlus._delete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Deletes this Vertex AI resource. WARNING: This deletion is permanent.\"\"\"\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_start_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1202\u001b[0m lro \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delete_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_started_against_resource_with_lro(\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelete\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, lro\n\u001b[1;32m   1205\u001b[0m )\n\u001b[1;32m   1206\u001b[0m lro\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py:1668\u001b[0m, in \u001b[0;36mModelServiceClient.delete_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1668\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;66;03m# Wrap the response in an operation future.\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m response \u001b[38;5;241m=\u001b[39m gac_operation\u001b[38;5;241m.\u001b[39mfrom_gapic(\n\u001b[1;32m   1677\u001b[0m     response,\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39moperations_client,\n\u001b[1;32m   1679\u001b[0m     empty_pb2\u001b[38;5;241m.\u001b[39mEmpty,\n\u001b[1;32m   1680\u001b[0m     metadata_type\u001b[38;5;241m=\u001b[39mgca_operation\u001b[38;5;241m.\u001b[39mDeleteOperationMetadata,\n\u001b[1;32m   1681\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Deploy_Model/venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 The model \"projects/522870214401/locations/us-west1/models/6110637026434875392\" can't be deleted because it's deployed or being deployed at the following endpoint(s): projects/522870214401/locations/us-west1/endpoints/5130308061063282688, projects/522870214401/locations/us-west1/endpoints/8381906992024780800, projects/522870214401/locations/us-west1/endpoints/9183547725696729088. Undeploy the model from all endpoints first and then delete it."
     ]
    }
   ],
   "source": [
    "# delete model\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Models following the order: undeploy -> delete endpoint -> delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<google.cloud.aiplatform.models.Endpoint object at 0x2a614ff10> \n",
      "resource name: projects/522870214401/locations/us-west1/endpoints/1634388850316935168]\n"
     ]
    }
   ],
   "source": [
    "endpoints = aiplatform.Endpoint.list()\n",
    "print(endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-west1-aiplatform.googleapis.com/]\n",
      "{'createTime': '2024-04-24T13:34:23.961099Z', 'dedicatedResources': {'machineSpec': {'machineType': 'e2-standard-4'}, 'maxReplicaCount': 1, 'minReplicaCount': 1}, 'displayName': 'paraphrase-MiniLM-L6-v2', 'id': '918140587704582144', 'model': 'projects/522870214401/locations/us-west1/models/4594612801871282176', 'modelVersionId': '1'}\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai endpoints describe 1634388850316935168 --project=rag-nick --region=us-west1 --format=\"value(deployedModels)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai endpoints undeploy-model 1634388850316935168 --project=rag-nick --region=us-west1 --deployed-model-id=918140587704582144"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
